# Основы работы с Greenplum

## Содержание
1. [Дистрибуция и создание таблиц](#дистрибуция-и-создание-таблиц)
   - [Обзор](#обзор)
   - [Цели](#цели)
   - [Структура базы данных](#структура-базы-данных)
   - [Ключевые особенности](#ключевые-особенности)
   - [Приобретенные навыки и компетенции](#приобретенные-навыки-и-компетенции)
2. [Интеграция с внешними системами](#интеграция-с-внешними-системами)
   - [Описание выполненной работы](#описание-выполненной-работы)
   - [Приобретенные навыки и компетенции](#приобретенные-навыки-и-компетенции-1)
3. [User Define Functions](#user-define-functions)
    - [Задача](#задача)
    - [Функции](#функции)
    - [Приобретенные навыки и компетенции](#приобретенные-навыки-и-компетенции-2)
4. [Apache Airflow](#apache-airflow)
    - [Задача](#задача-1)
    - [Описание](#описание)
    - [Приобретенные навыки и компетенции](#приобретенные-навыки-и-компетенции-3)


## Дистрибуция и создание таблиц

### Обзор

"Дистрибуция и создание таблиц" — это проект, сосредоточенный на разработке оптимизированной базы данных для отслеживания продаж, каналов распределения и данных о продуктах. Проект моделирует реальный сценарий, в котором эффективное хранение, распределение и анализ больших объемов данных имеют критическое значение. База данных использует такие техники, как партиционирование, сжатие данных и распределение для повышения производительности и масштабируемости.

Этот проект идеален для изучения проектирования баз данных, хранения данных и аналитики в контексте данных о продажах.

### Цели

1. **Проектирование базы данных**: Изучение проектирования и реализации фактических и размерных таблиц с использованием стратегий партиционирования и распределения.
2. **Оптимизация производительности**: Изучение того, как партиционирование, сжатие данных и репликация могут улучшить скорость запросов и эффективность хранения для больших объемов данных.

### Структура базы данных

База данных следует структуре "звезда" с таблицами фактов и справочниками, организованными следующим образом:

#### Таблицы фактов:
- **sales** (`std7_170.sales`):
  - Эта таблица отслеживает транзакции по продажам.
  - **Столбцы**: `date`, `region`, `material`, `distr_chan`, `quantity`, `check_nm`, `check_pos`
  - **Партиционирование**: По `date` (на основе диапазона).
  - **Распределение**: По `check_nm`.
  - **Сжатие**: `zstd`.

- **plan** (`std7_170.plan`):
  - Эта таблица отслеживает данные о планировании продаж.
  - **Столбцы**: `date`, `region`, `matdirec`, `quantity`, `distr_chan`
  - **Партиционирование**: По `date` (на основе диапазона).
  - **Распределение**: Случайное.
  - **Сжатие**: `zstd`.

#### Таблицы справочников:
- **channel** (`std7_170.chanel`):
  - Эта таблица описывает каналы распределения.
  - **Столбцы**: `distr_chan`, `txtsh`
  - **Распределение**: Реплицированное.

- **price** (`std7_170.price`):
  - Эта таблица отслеживает информацию о ценах на продукцию.
  - **Столбцы**: `material`, `region`, `distr_chan`, `price`
  - **Распределение**: Реплицированное.

- **product** (`std7_170.product`):
  - Эта таблица предоставляет детали о продукции.
  - **Столбцы**: `material`, `asgrp`, `brand`, `matcateg`, `matdirec`, `txt`
  - **Распределение**: Реплицированное.

- **region** (`std7_170.region`):
  - Эта таблица описывает регионы.
  - **Столбцы**: `region`, `txt`
  - **Распределение**: Реплицированное.

### Ключевые особенности

1. **Партиционирование и распределение**:
   - Фактические таблицы `sales` и `plan` партиционированы по `date` для оптимизации временных запросов.
   - Распределения используются для балансировки нагрузки запросов.

2. **Сжатие данных**:
   - Применено сжатие `zstd` для экономии места и оптимизации производительности.

### Приобретенные навыки и компетенции

В ходе выполнения этого проекта я развил следующие навыки и компетенции:

- **Проектирование баз данных**: Получил опыт проектирования и реализации фактических и размерных таблиц, ориентированных на анализ данных о продажах.
- **Оптимизация запросов**: Улучшил навыки написания эффективных SQL-запросов и понимания того, как партиционирование и распределение могут повысить производительность.
- **Аналитическое мышление**: Углубил способность анализировать бизнес-требования и преобразовывать их в хорошо структурированное проектирование базы данных.

## Интеграция с внешними системами

### Описание выполненной работы

В этом разделе я описываю интеграцию с внешними системами через создание внешних таблиц в Greenplum. Цели и результаты следующие:

1. **Создание внешних таблиц с использованием протокола PXF**:
   - **Цель**: Обеспечить доступ к данным из существующих таблиц Postgres (`gp.plan` и `gp.sales`) в среде Greenplum.
   - **Результат**: Успешно созданы внешние таблицы, которые облегчают извлечение данных, что позволяет бесшовно интегрировать данные о продажах и планировании.
   - **Описание**: Протокол PXF (Pivotal Extension Framework) используется для подключения Greenplum к внешним источникам данных, что позволяет выполнять запросы к реляционным базам данных через JDBC.

2. **Создание внешних таблиц с использованием протокола gpfdist**:
   - **Цель**: Получение и управление данными, хранящимися в CSV-файлах (`price`, `chanel`, `product`, и `region`) с использованием протокола gpfdist.
   - **Результат**: Созданы внешние таблицы, позволяющие эффективно загружать данные из этих CSV-файлов в Greenplum, улучшая гибкость и доступность источников данных для анализа.
   - **Описание**: Протокол gpfdist позволяет Greenplum считывать данные из файлов, доступных по HTTP, что обеспечивает эффективную загрузку данных из распределённых файловых систем или облачного хранилища.

### Приобретенные навыки и компетенции

В ходе выполнения этой интеграционной работы я развил следующие навыки и компетенции:

- **Понимание интеграции данных**: Получил знания о том, как интегрировать разные источники данных с использованием различных протоколов, что повысило моё понимание рабочих процессов данных.
- **Профессионализм в Greenplum**: Улучшил свои навыки создания внешних таблиц и использования функций Greenplum для эффективного управления данными.
- **Знание протоколов доступа к данным**: Научился внедрять и использовать протоколы PXF и gpfdist, расширив свои знания о методах доступа к данным.
- **Аналитические навыки**: Улучшил свои способности анализировать структуры данных и точки интеграции, что привело к более эффективным стратегиям извлечения и анализа данных.

## User Define Functions

### Задача

Организация эффективного процесса загрузки данных с использованием ELT-подхода. Управление процессами загрузки данных, включая полное обновление справочников, загрузку витрин и обработку данных по партициям.

### Функции

**1. Функция `f_load_full(p_table text, p_file_name text)`**
- **Описание:**
Полная перезапись таблицы с использованием DELETE или TRUNCATE с последующим вставлением всех записей.
- **Особенности:**
Используется TRUNCATE + INSERT для быстрой перезаписи таблицы, однако операция блокирует таблицу с уровнем ACCESS EXCLUSIVE.

**2. Функция `f_load_mart(p_month varchar)`**
- **Описание:**
Загрузка данных в витрину для указанного месяца.

**3. Функция `f_load_simple_delta_partition(p_table text, p_partition_key text, p_start_date timestamp, p_end_date timestamp, p_pxf_table text, p_user_id text, p_pass text)`**
- **Описание:**
Подмена данных в партициях на дневной, недельной или месячной основе. Эта функция полезна при работе с большими таблицами, которые обычно партиционируются по дате.

**4. Функция `f_load_write_log(p_log_type text, p_log_message text, p_location text)`**
- **Описание:**
Запись логов загрузки данных.

### Приобретенные навыки и компетенции

- Разработка и внедрение UDF для автоматизации ETL процессов.
- Создание функций для обновления данных в таблицах с учетом партиционирования.
- Оптимизация процессов загрузки и обработки данных с помощью TRUNCATE + INSERT.

## Apache Airflow

### Задача

Автоматизация процессов ELT с помощью Apache Airflow, включая загрузку данных в таблицы, партиционирование и создание витрин.

### Описание

1. **Инициализация DAG**:
   - Настройка DAG для автоматизации ETL процессов с использованием Airflow.
   - Определены процессы загрузки партиций, полные перезаписи таблиц и создание витрин данных.
2. **Группы задач:**:
   - `load_delta_part_table`: Загрузка данных по партициям за указанный период.
   - `full_load`: Полная перезапись таблиц справочников.
   - `create_data_mart`: Загрузка данных в витрину за указанный месяц.

### Приобретенные навыки и компетенции

Приобретенные навыки:

- Настройка DAG для автоматизации ELT процессов в Airflow.
- Управление и мониторинг задач, связанных с загрузкой данных, созданием витрин и обновлением таблиц.
- Оптимизация последовательной и параллельной обработки данных, настройка зависимостей между задачами и контроль выполнения DAG.